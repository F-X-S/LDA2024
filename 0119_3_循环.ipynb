{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入文件及分词\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "output_path = 'C:\\\\Users\\\\F7910\\\\Desktop\\\\LDA2024\\\\output'\n",
    "data=pd.read_excel('C:\\\\Users\\\\F7910\\\\Desktop\\\\LDA2024\\\\data\\\\data5.xlsx').astype(str)  #content type\n",
    "Tingyong = open('C:\\\\Users\\\\F7910\\Desktop\\\\LDA_database\\\\TingYongCi', encoding='gbk')\n",
    "file_path = 'C:\\\\Users\\\\F7910\\\\Desktop\\\\LDA_database\\\\CiQianRu\\\\glove_2.txt'\n",
    "os.chdir(output_path)\n",
    "\n",
    "# 构建人工停用词典 Tingyong_1\n",
    "Tingyong_1=[]\n",
    "for line in Tingyong:\n",
    "    Tingyong_1.append(line.strip())\n",
    "\n",
    "\n",
    "# 分词操作\n",
    "def english_word_cut(mytext):\n",
    "    cutwords1 = word_tokenize(mytext)\n",
    "    cutwords1 = [word.lower() for word in cutwords1]\n",
    "    cutwords2 = [word for word in cutwords1 if word not in Tingyong_1]\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    cutwords3 = [word for word in cutwords2 if word not in stops]\n",
    "    cutwords3 = [word for word in cutwords3 if word.isalpha()]\n",
    "    word_list = []\n",
    "    for word in cutwords3:\n",
    "        word_list.append(word)      \n",
    "    return (\" \").join(word_list)\n",
    "\n",
    "data[\"content_cutted\"] = data.content.apply(english_word_cut)\n",
    "\n",
    "######################\n",
    "corpus_tokenized = [doc.split() for doc in data[\"content_cutted\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入词向量及哈希表\n",
    "data_cqr_1 = pd.read_csv(file_path,sep=' ',on_bad_lines='skip',header=0,engine=\"python\")\n",
    "data_cqr_1 = data_cqr_1.transpose()  # 转置\n",
    "data_cqr_1.columns = data_cqr_1.iloc[0]  # 将第一行设置为列名\n",
    "data_cqr_1 = data_cqr_1.drop(data_cqr_1.index[0]) # 删除第一行\n",
    "data_cqr_2 = data_cqr_1.values # 只取数字（只保留列向量）\n",
    "# emb_matrix_l = data_cqr_2.astype(float) # 将向量值变成浮点类型\n",
    "# emb_matrix_h = emb_matrix_l.T # 转置变成行向量\n",
    "# 列表套列表 其中每一列表是分好词的一篇文章\n",
    "\n",
    "# 哈希\n",
    "def build_hash_table(lst):\n",
    "    hash_table = {}\n",
    "    for i, s in enumerate(lst):\n",
    "        hash_table[s] = i\n",
    "    return hash_table\n",
    "def search_in_hash_table(hash_table, s):\n",
    "    if s in hash_table:\n",
    "        return hash_table[s]\n",
    "    else:\n",
    "        return -1\n",
    "def get_string_from_hash_table(hash_table, index):\n",
    "    for key, value in hash_table.items():\n",
    "        if value == index:\n",
    "            return key\n",
    "    # return None\n",
    "str_list = list(data_cqr_1.columns)\n",
    "# 构建哈希表\n",
    "hash_table_1 = build_hash_table(str_list)\n",
    "\n",
    "cqr_all_1 = set(data_cqr_1.columns)\n",
    "data_all_2 = []\n",
    "for cen in data[\"content_cutted\"]:\n",
    "    for ci in cen.split():\n",
    "        data_all_2.append(ci)\n",
    "data_all_3 = set(data_all_2)\n",
    "\n",
    "str_list = list(data_cqr_1.columns)\n",
    "\n",
    "data_cqr_3 = [word for word in str_list if str(word).isalpha()]\n",
    "ind = []\n",
    "for word in data_cqr_3:\n",
    "    ind.append(search_in_hash_table(hash_table_1,word))\n",
    "cqr_f_1 = np.take(data_cqr_2,ind,axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入lda相关包\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入k-means相关包\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主体循环\n",
    "np.random.seed(8)\n",
    "\n",
    "topic_num = []\n",
    "coherence_score = []\n",
    "perplexity_score = []\n",
    "silhouette_avg = []\n",
    "ch_score = []\n",
    "db_score = []\n",
    "\n",
    "for topic in range(4,20):\n",
    "    topic_n = topic\n",
    "    # 创建 LDA 主题模型\n",
    "    dictionary = Dictionary(corpus_tokenized)\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in corpus_tokenized]\n",
    "    lda_model = LdaModel(corpus=corpus_bow,    # 语料库\n",
    "                        num_topics=topic_n,   # 主题数\n",
    "                        id2word=dictionary,   # 词典，将单词转换为数字索引\n",
    "                        iterations=100,  # 迭代次数\n",
    "                        chunksize = 50 ,\n",
    "                        # 在训练LDA模型时，语料库需要被切割成多少个块。每个块都会被用于训练LDA模型中的一次迭代\n",
    "                        passes=5,     # 训练时语料库需要被重复遍历的次数\n",
    "                        random_state = 2\n",
    "                        )\n",
    "\n",
    "    # 计算主题一致性\n",
    "    coherence_model = CoherenceModel(model=lda_model, \n",
    "                                     texts=corpus_tokenized, \n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v',\n",
    "                                     window_size = 30\n",
    "                                    )\n",
    "\n",
    "    # 计算主题困惑度\n",
    "    perplexity_score.append(lda_model.log_perplexity(chunk = corpus_bow))\n",
    "\n",
    "\n",
    "    topic_num.append(topic)\n",
    "    coherence_score.append(coherence_model.get_coherence())\n",
    "\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus_bow, dictionary)\n",
    "    pyLDAvis.display(vis)\n",
    "    pyLDAvis.save_html(vis, 'lda_pass' + str(topic_n)+'.html')\n",
    "\n",
    "\n",
    "    \n",
    "    t = []\n",
    "    import re\n",
    "    for idx,topic in lda_model.show_topics(num_topics=-1):\n",
    "        for w in topic.split():\n",
    "            # print(w)\n",
    "            letters_only = re.sub(r'[^a-zA-Z]', '', w)\n",
    "            # print(letters_only)\n",
    "            t.append(letters_only)\n",
    "    t = list(filter(None,t))\n",
    "\n",
    "    ind_k = []\n",
    "    for word in t:\n",
    "        ind_k.append(search_in_hash_table(hash_table_1,word))\n",
    "    cqr_f_k = np.take(data_cqr_2,ind_k,axis=1).astype(float)\n",
    "    \n",
    "    data = cqr_f_k\n",
    "\n",
    "    # 创建一个K-means模型，假设你想要将数据聚类成簇\n",
    "    kmeans = KMeans(n_clusters=topic_n, n_init='auto')\n",
    "\n",
    "    # 将模型拟合到数据\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    # 获取簇中心和标签\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # 可视化数据点和簇中心\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis', marker='o', label='Data Points')\n",
    "    plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', s=200, label='Cluster Centers')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.legend()\n",
    "    plt.title('K-means Clustering')\n",
    "\n",
    "    file_name = 'k_means_clustering_{}.png'.format(topic_n)  \n",
    "    plt.savefig(file_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate clustering performance\n",
    "    silhouette_avg.append(silhouette_score(data, kmeans.labels_))\n",
    "    ch_score.append(calinski_harabasz_score(data, kmeans.labels_))\n",
    "    db_score.append(davies_bouldin_score(data, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制coherence_score\n",
    "x = topic_num\n",
    "y = coherence_score\n",
    "# 创建折线图\n",
    "plt.plot(x, y, label='coherence_score')\n",
    "# 添加标签和标题\n",
    "plt.xlabel('topic_num')\n",
    "plt.ylabel('coherence_score')\n",
    "plt.title('coherence_score')\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制perplexity_score\n",
    "x = topic_num\n",
    "y = [np.exp(-perplexity) for perplexity in perplexity_score]\n",
    "# 创建折线图\n",
    "plt.plot(x, y, label='perplexity_score')\n",
    "# 添加标签和标题\n",
    "plt.xlabel('topic_num')\n",
    "plt.ylabel('perplexity_score')\n",
    "plt.title('Perplexity Score')\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制silhouette_avg\n",
    "x = topic_num\n",
    "y = silhouette_avg\n",
    "# 创建折线图\n",
    "plt.plot(x, y, label='silhouette_avg')\n",
    "# 添加标签和标题\n",
    "plt.xlabel('topic_num')\n",
    "plt.ylabel('silhouette_avg')\n",
    "plt.title('silhouette_avg')\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制ch_score\n",
    "x = topic_num\n",
    "y = ch_score\n",
    "# 创建折线图\n",
    "plt.plot(x, y, label='ch_score')\n",
    "# 添加标签和标题\n",
    "plt.xlabel('topic_num')\n",
    "plt.ylabel('ch_score')\n",
    "plt.title('ch_score')\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
